{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import time, random, math, string\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "SEED = 1225\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "from utils.data import processing_data_EUR_Lex, processing_data_wiki31k, processing_data_Amazon670k, processing_data_AmazonCat13k\n",
    "from utils.plot import train_valid_loss\n",
    "from utils.embedding import src_embedding_glove, tgt_embedding_glove\n",
    "from model.seq2seq import Seq2Seq\n",
    "from model.seq2seq_conv import Seq2Seq as Seq2SeqConv\n",
    "from model.loss import DynamicHungarianLossAssignAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 0\n",
    "device = torch.device('cuda:{}'.format(gpu_id) if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name or \"bias\" in name:\n",
    "            nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train(model, iterator, optimizer, loss_func, teacher_forcing_ratio, clip = None, lambda_embedding=0.1):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, lengths = batch.src\n",
    "        tgt = batch.tgt\n",
    "        # src = [src_len, batch_size]\n",
    "        # tgt = [tgt_len, batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if isinstance(loss_func, nn.CrossEntropyLoss):\n",
    "            output = model(src, lengths, tgt, teacher_forcing_ratio)\n",
    "            output_dim = output.shape[-1]\n",
    "            # output = [tgt_len, batch_size, output_dim]\n",
    "            # transfrom output : flatten the output into 2 dim.\n",
    "            output = output.view(-1, output_dim)\n",
    "            tgt = tgt[1:].view(-1) # exclude SOS\n",
    "            # output = [tgt_len * batch_size, output_dim]\n",
    "            # tgt = [tgt_len * batch_size]\n",
    "            loss = loss_func(output, tgt)\n",
    "        elif isinstance(loss_func, DynamicHungarianLossAssignAll):\n",
    "            output = model(src, lengths, tgt, teacher_forcing_ratio)\n",
    "            output = output.transpose(0,1)\n",
    "            tgt = tgt[1:].transpose(0,1)\n",
    "            loss = loss_func(output, tgt, model.decoder.embedding)\n",
    "                \n",
    "        loss.backward()\n",
    "        # clip\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, loss_func, lambda_embedding=0.1):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            src, lengths= batch.src\n",
    "            tgt = batch.tgt\n",
    "            \n",
    "            if isinstance(loss_func, nn.CrossEntropyLoss):\n",
    "                output = model(src, lengths, tgt, 0) # turn off teacher forcing.\n",
    "                output_dim = output.shape[-1]\n",
    "                # output = [tgt_len, batch_size, output_dim]\n",
    "                # transfrom output : flatten the output into 2 dim.\n",
    "                output = output.view(-1, output_dim)\n",
    "                tgt = tgt[1:].view(-1) # exclude SOS\n",
    "                # output = [tgt_len * batch_size, output_dim]\n",
    "                # tgt = [tgt_len * batch_size]\n",
    "                loss = loss_func(output, tgt)\n",
    "            elif isinstance(loss_func, DynamicHungarianLossAssignAll):\n",
    "                output = model(src, lengths, tgt, 0) # turn off teacher forcing.\n",
    "                output = output.transpose(0,1)\n",
    "                tgt = tgt[1:].transpose(0,1)\n",
    "                loss = loss_func(output, tgt, model.decoder.embedding)\n",
    "                    \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# a function that used to tell us how long an epoch takes.\n",
    "def epoch_time(start_time, end_time):\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time  / 60)\n",
    "    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n",
    "    return  elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_evaluate(iterator, model_result_path, SRC, TGT, con, suffix=\"\", *, use_final=False, beam=False, beam_width=5):\n",
    "    INIT_IDX = TGT.vocab.stoi[TGT.init_token]\n",
    "    PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "    EOS_IDX = TGT.vocab.stoi[TGT.eos_token]\n",
    "    \n",
    "    if hasattr(con, \"dl_conv\") and con.dl_conv:\n",
    "        model = Seq2SeqConv(con)\n",
    "    else:\n",
    "        model = Seq2Seq(con)\n",
    "    \n",
    "    if use_final:\n",
    "        suffix +=\"_final\"\n",
    "        model.load_state_dict(torch.load('result/{}_final.pt'.format(model_result_path), map_location=f\"cuda:{gpu_id}\"))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load('result/{}.pt'.format(model_result_path), map_location=f\"cuda:{gpu_id}\"))\n",
    "    \n",
    "    model.to(device)\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    tgt_true_all = []\n",
    "    tgt_pred_all = []\n",
    "    \n",
    "    predict_file = f\"result/{model_result_path}_test{suffix}.txt\"\n",
    "    if os.path.exists(predict_file):\n",
    "        os.remove(predict_file)\n",
    "    \n",
    "    # evalute metrics\n",
    "    TP = torch.zeros(len(TGT.vocab), dtype=torch.long, device=device)\n",
    "    FP = torch.zeros(len(TGT.vocab), dtype=torch.long, device=device)\n",
    "    FN = torch.zeros(len(TGT.vocab), dtype=torch.long, device=device)\n",
    "    \n",
    "    TP_p1 = 0\n",
    "    TP_p3 = 0\n",
    "    TP_p5 = 0\n",
    "    \n",
    "    # example based\n",
    "    eb_precision = 0\n",
    "    eb_recall = 0\n",
    "    eb_f1 = 0\n",
    "    \n",
    "    output_dim = len(TGT.vocab)\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for k, batch in enumerate(iterator):\n",
    "            \n",
    "            src, lengths = batch.src\n",
    "            tgt = batch.tgt\n",
    "            # src : [seq_len, batch_size]\n",
    "                \n",
    "            result = None\n",
    "            if beam:\n",
    "                result = model.beam_sample(src, lengths, INIT_IDX, PAD_IDX, EOS_IDX, con.mask, beam_width=beam_width)\n",
    "            else:\n",
    "                output = model.sample(src, lengths, INIT_IDX, PAD_IDX, EOS_IDX, con.mask)\n",
    "                # output = [seq_len, batch_size, output_dim]\n",
    "                output = output.argmax(2).T\n",
    "                # output: [batch_size, seq_len]\n",
    "                result = output.cpu().numpy().tolist()\n",
    "            \n",
    "            src = src.T.cpu().numpy().tolist()\n",
    "            tgt_tensor = tgt[1:].T\n",
    "            # tgt_tensor: [batch_size, seq_len]\n",
    "            tgt = tgt_tensor.cpu().numpy().tolist()\n",
    "            \n",
    "            with open(predict_file, 'a') as f:\n",
    "                for i in range(len(result)):\n",
    "                    # symptom\n",
    "                    src_one = src[i]\n",
    "                    src_true = []\n",
    "                    for j in range(len(src_one)):\n",
    "                        src_true.append(SRC.vocab.itos[src_one[j]])\n",
    "                    # true labels\n",
    "                    tgt_true_one = tgt[i]\n",
    "                    tgt_true = []\n",
    "                    for k in range(len(tgt_true_one)):\n",
    "                        tgt_true.append(TGT.vocab.itos[tgt_true_one[k]])\n",
    "                    # predicted herb\n",
    "                    one = result[i]\n",
    "                    tgt_result=[]\n",
    "                    for j in range(len(one)):\n",
    "                        tgt_result.append(TGT.vocab.itos[one[j]])\n",
    "                    f.write(\"{} | {} | {} \\n\".format(\" \".join(src_true), \" \".join(tgt_true), \" \".join(tgt_result)))\n",
    "                    \n",
    "                    # if con.cardinality:\n",
    "                    #     output_i = output[i][1:]\n",
    "                    # else:\n",
    "                    #     output_i = output[i]\n",
    "                    output_i = output[i]\n",
    "                    # eos_idx is not 0 when vocab have eos, make sure build_vocab has specials_first=true\n",
    "                    if EOS_IDX != 0:\n",
    "                        first_eos_index_o = (output_i==EOS_IDX).nonzero(as_tuple=True)[0]\n",
    "                        if first_eos_index_o.size()[0] > 0:\n",
    "                            first_eos_index_o = first_eos_index_o[0].item()\n",
    "                            output_i = output_i[:first_eos_index_o]\n",
    "                    \n",
    "                    output_i_onehot = torch.zeros(output_dim, dtype=torch.bool, device=device).scatter_(0, output_i, 1)\n",
    "                    \n",
    "                    # if con.cardinality:\n",
    "                    #     tgt_tensor_i = tgt_tensor[i][1:]\n",
    "                    # else:\n",
    "                    #     tgt_tensor_i = tgt_tensor[i]\n",
    "                    tgt_tensor_i = tgt_tensor[i]\n",
    "                    \n",
    "                    # eos_idx is not 0 when vocab have eos, make sure build_vocab has specials_first=true\n",
    "                    if EOS_IDX != 0:\n",
    "                        first_eos_index_t = (tgt_tensor_i==EOS_IDX).nonzero(as_tuple=True)[0]\n",
    "                        if first_eos_index_t.size()[0] > 0:\n",
    "                            first_eos_index_t = first_eos_index_t[0].item()\n",
    "                            tgt_tensor_i = tgt_tensor_i[:first_eos_index_t]\n",
    "                        \n",
    "                    tgt_tensor_i_onehot = torch.zeros(output_dim, dtype=torch.bool, device=device).scatter_(0, tgt_tensor_i, 1)\n",
    "                    \n",
    "                    # for micro f1 score\n",
    "                    TP_i = torch.logical_and(output_i_onehot, tgt_tensor_i_onehot, out=torch.empty(output_dim, dtype=torch.bool, device=device))\n",
    "                    TP = TP + TP_i\n",
    "                    FP_i = torch.logical_and(output_i_onehot, torch.logical_not(tgt_tensor_i_onehot), out=torch.empty(output_dim, dtype=torch.bool, device=device))\n",
    "                    FP = FP + FP_i\n",
    "                    FN_i = torch.logical_and(torch.logical_not(output_i_onehot), tgt_tensor_i_onehot, out=torch.empty(output_dim, dtype=torch.bool, device=device))\n",
    "                    FN = FN + FN_i\n",
    "                    # for precision@k\n",
    "                    output_i_indices = output_i.cpu().numpy().tolist()\n",
    "                    output_i_indices_nopad = [i for i in output_i_indices if i != PAD_IDX]\n",
    "                    tgt_i = set(tgt_tensor_i.cpu().numpy())\n",
    "                    TP_p1 += len(set(output_i_indices_nopad[:1]) & tgt_i)\n",
    "                    TP_p3 += len(set(output_i_indices_nopad[:3]) & tgt_i)\n",
    "                    TP_p5 += len(set(output_i_indices_nopad[:5]) & tgt_i)\n",
    "                    total += 1\n",
    "                    \n",
    "                    # example based\n",
    "                    TP_i[PAD_IDX] = 0\n",
    "                    FP_i[PAD_IDX] = 0\n",
    "                    FN_i[PAD_IDX] = 0\n",
    "                    TP_i_sum = torch.sum(TP_i).item()\n",
    "                    FP_i_sum = torch.sum(FP_i).item()\n",
    "                    FN_i_sum = torch.sum(FN_i).item()\n",
    "                    P_i_sum = TP_i_sum + FP_i_sum\n",
    "                    if P_i_sum !=0:\n",
    "                        eb_precision += TP_i_sum / P_i_sum\n",
    "                    G_i_sum = TP_i_sum + FN_i_sum\n",
    "                    if G_i_sum !=0:\n",
    "                        eb_recall += TP_i_sum / G_i_sum\n",
    "                    F1_D_i_sum = 2*TP_i_sum + FP_i_sum + FN_i_sum\n",
    "                    if F1_D_i_sum !=0:\n",
    "                        eb_f1 += 2*TP_i_sum / F1_D_i_sum\n",
    "                    \n",
    "    # ignore pad\n",
    "    TP[PAD_IDX] = 0\n",
    "    FP[PAD_IDX] = 0\n",
    "    FN[PAD_IDX] = 0\n",
    "    TP_sum = torch.sum(TP).item()\n",
    "    f1_micro = 2*TP_sum / (2*TP_sum + torch.sum(FP).item() + torch.sum(FN).item())\n",
    "    precision_micro = TP_sum / (TP_sum + torch.sum(FP).item())\n",
    "    recall_micro = TP_sum / (TP_sum + torch.sum(FN).item())\n",
    "    \n",
    "    # macro\n",
    "    special_num = 3 # unk,pad, init\n",
    "    if EOS_IDX != 0: # eos_idx is not 0 when vocab have eos, make sure build_vocab has specials_first=true\n",
    "        special_num += 1\n",
    "    \n",
    "    P = torch.add(TP, FP)\n",
    "    P_non = P.eq(0)\n",
    "#     P_non_num = torch.sum(P_non).item()\n",
    "    \n",
    "    # ground truth\n",
    "    G = torch.add(TP, FN)\n",
    "    G_non = G.eq(0) \n",
    "#     G_non_num = torch.sum(G_non).item()\n",
    "    \n",
    "    # consider labels not appreared in test set\n",
    "#     macro_p_non_empty = torch.sum(torch.div(TP, torch.add(P, P_non))).item() / (TP.size()[0] - P_non_num)\n",
    "#     macro_r_non_empty = torch.sum(torch.div(TP, torch.add(G, G_non))).item() / (TP.size()[0] - G_non_num)\n",
    "    # is not correct for macro\n",
    "#     macro_f1 = 2 / (1/macro_p + 1/macro_r)\n",
    "    \n",
    "    macro_p = torch.sum(torch.div(TP, torch.add(P, P_non))).item() / (TP.size()[0] - special_num)\n",
    "    macro_r = torch.sum(torch.div(TP, torch.add(G, G_non))).item() / (TP.size()[0] - special_num)\n",
    "#     macro_f1_all = 2 / (1/macro_p_all + 1/macro_r_all)\n",
    "    \n",
    "    # macro f1 denominator\n",
    "    F1_D = 2*TP + FN + FP\n",
    "    F1_D_non = F1_D.eq(0)\n",
    "#     F1_D_non_num = torch.sum(F1_D_non).item()\n",
    "    # consider labels not appeared in test set\n",
    "#     macro_f1_non_empty = torch.sum(torch.div(2*TP, torch.add(F1_D, F1_D_non))).item() / (TP.size()[0] - F1_D_non_num)\n",
    "    \n",
    "    macro_f1 = torch.sum(torch.div(2*TP, torch.add(F1_D, F1_D_non))).item() / (TP.size()[0] - special_num)\n",
    "    \n",
    "    # weighted macro\n",
    "    weight = G/torch.sum(G).item()\n",
    "    weighted_macro_p = torch.dot(torch.div(TP, torch.add(P, P_non)), weight)\n",
    "    weighted_macro_r = torch.dot(torch.div(TP, torch.add(G, G_non)), weight)\n",
    "    weighted_macro_f1 = torch.dot(torch.div(2*TP, torch.add(F1_D, F1_D_non)), weight)\n",
    "    \n",
    "    # example based \n",
    "    eb_precision = eb_precision/total\n",
    "    eb_recall = eb_recall/total\n",
    "    eb_f1 = eb_f1/total\n",
    "    \n",
    "    #top k\n",
    "    p1 = TP_p1 / total\n",
    "    p3 = TP_p3 / (total * 3)\n",
    "    p5 = TP_p5 / (total * 5)\n",
    "    \n",
    "    evaluation = f'''\n",
    "p1:{p1}\\np3:{p3}\\np5:{p5}\\n\n",
    "precision_micro:{precision_micro}\\nrecall_micro:{recall_micro}\\nf1_micro{f1_micro}\\n\n",
    "macro_p:{macro_p}\\nmacro_r:{macro_r}\\nmacro_f1:{macro_f1}\\n\n",
    "weighted_macro_p:{weighted_macro_p}\\nweighted_macro_r:{weighted_macro_r}\\nweighted_macro_f1:{weighted_macro_f1}\\n\n",
    "eb_precision:{eb_precision}\\neb_recall:{eb_recall}\\neb_f1:{eb_f1}\\n\n",
    "    '''\n",
    "    with open(f\"result/{model_result_path}_evaluation{suffix}_new.txt\",'w') as f:\n",
    "        f.write(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute weight\n",
    "def class_balance(freq, beta=0.9):\n",
    "    return (1-beta)/(1-beta**freq)\n",
    "\n",
    "def get_class_balanced_weight(TGT, beta=0.9):\n",
    "    vocab_size = len(TGT.vocab)\n",
    "    weight = torch.ones(vocab_size, device=device)\n",
    "\n",
    "    for i in range(vocab_size):\n",
    "        token = TGT.vocab.itos[i]\n",
    "        if token == TGT.unk_token or token == TGT.pad_token or token == TGT.init_token:\n",
    "            weight[i] = (1-beta)*0.2\n",
    "        else:\n",
    "            weight[i] = class_balance(TGT.vocab.freqs[token], beta)\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/OTSeq2Set.json\", 'r') as f:\n",
    "# with open(\"config/baselines.json\", 'r') as f:\n",
    "    config = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Struct:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_name, conf_list in config.items():\n",
    "    \n",
    "    if data_name == \"EUR-Lex\":\n",
    "        processing_data = processing_data_EUR_Lex\n",
    "    elif data_name == \"Wiki31k\":\n",
    "        processing_data = processing_data_wiki31k\n",
    "    elif data_name ==\"Amazon670k\":\n",
    "        processing_data = processing_data_Amazon670k\n",
    "    elif data_name == \"AmazonCat13k\":\n",
    "        processing_data = processing_data_AmazonCat13k\n",
    "    else:\n",
    "        raise Exception(\"Data set don't exists!\")\n",
    "\n",
    "    for conf in conf_list:\n",
    "        if conf.get(\"finish\") == False:\n",
    "            con = Struct(**conf)\n",
    "            print(f\"dataset:{data_name},model:{con.model}:{con.model_number}\")\n",
    "            # load data\n",
    "            for_CE = True if con.loss_func in (\"CE\", \"DynamicHungarianLoss\") else False\n",
    "            \n",
    "            tgt_sort = True if hasattr(con, \"tgt_sort\") and con.tgt_sort else False\n",
    "            \n",
    "            if hasattr(con, \"max_src_len\"):\n",
    "                train_iter, valid_iter, test_iter, SRC, TGT = processing_data(device, max_src_len=con.max_src_len, include_lengths=True, batch_size=con.batch_size,\n",
    "                                                                          for_CE=for_CE, valid_split=con.valid_split, tgt_sort=tgt_sort)\n",
    "            else:\n",
    "                train_iter, valid_iter, test_iter, SRC, TGT = processing_data(device, include_lengths=True, batch_size=con.batch_size,\n",
    "                                                                              for_CE=for_CE, valid_split=con.valid_split, tgt_sort=tgt_sort)\n",
    "                                                                        \n",
    "            PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "            INIT_IDX = TGT.vocab.stoi[TGT.init_token]\n",
    "            EOS_IDX = TGT.vocab.stoi[TGT.eos_token]\n",
    "            print(f\"pad_idx:{PAD_IDX},init_idx:{INIT_IDX},eos_idx:{EOS_IDX}\")\n",
    "\n",
    "            \n",
    "            INPUT_DIM = len(SRC.vocab)\n",
    "            OUTPUT_DIM = len(TGT.vocab)\n",
    "            con.src_vocab_size = INPUT_DIM\n",
    "            con.tgt_vocab_size = OUTPUT_DIM\n",
    "            \n",
    "            # initialize model.\n",
    "            if hasattr(con, \"dl_conv\") and con.dl_conv:\n",
    "                model = Seq2SeqConv(con).to(device)\n",
    "            else:\n",
    "                model = Seq2Seq(con).to(device)\n",
    "            model.apply(init_weights)\n",
    "            \n",
    "            if con.src_glove:\n",
    "                model.encoder.embedding = src_embedding_glove(device, SRC)\n",
    "\n",
    "            if con.tgt_embedding == \"glove\":\n",
    "                print(\"tgt_embedding use: glove\")\n",
    "                model.decoder.embedding = tgt_embedding_glove(device, TGT)\n",
    "                \n",
    "            print(model)\n",
    "            print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "            if hasattr(model.decoder, \"bottleneck1\"):\n",
    "                print(f'The bottleneck1 has {count_parameters(model.decoder.bottleneck1):,} trainable parameters')\n",
    "                \n",
    "            if hasattr(model, \"dl_conv\"):\n",
    "                print(f'The dl_conv has {count_parameters(model.dl_conv):,} trainable parameters')\n",
    "                \n",
    "            optimizer = optim.Adam(model.parameters(), lr=con.learning_rate)\n",
    "            \n",
    "            if con.use_CosineAnnealingLR:\n",
    "                scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=con.N_EPOCHS)\n",
    "            else:\n",
    "                scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                                 'min', \n",
    "                                                                 factor=0.5,\n",
    "                                                                 patience=3,\n",
    "                                                                 verbose=1,\n",
    "                                                                 min_lr = 1e-10)\n",
    "            # loss func\n",
    "            if con.loss_func == \"CE\":\n",
    "                loss_func = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "            elif con.loss_func == \"DynamicHungarianLossAssignAll\":\n",
    "                loss_func = DynamicHungarianLossAssignAll(PAD_IDX, con.ignore_index, con.assign_all_pre, con.empty_weight, con.lambda_embedding,\n",
    "                                                         con.ipot_E_non_empty, con.ipot_E_first_n_pre)\n",
    "                \n",
    "            model_name = \"_\".join([data_name, con.model, con.loss_func])\n",
    "            # train\n",
    "            model_dir = f\"result/{model_name}\"\n",
    "            if not os.path.exists(model_dir):\n",
    "                os.mkdir(model_dir)\n",
    "            model_result_path = f\"{model_name}/{con.model_number}\"\n",
    "            best_valid_loss = float('inf')\n",
    "\n",
    "            train_loss_list = []\n",
    "            valid_loss_list = []\n",
    "            result_data = {}\n",
    "            total_time = 0\n",
    "            for epoch in range(con.N_EPOCHS):\n",
    "\n",
    "                start_time = time.time()\n",
    "                \n",
    "                train_loss = train(model, train_iter, optimizer, loss_func, con.teacher_forcing_ratio, con.CLIP)\n",
    "\n",
    "                end_time = time.time()\n",
    "                \n",
    "                total_time += end_time - start_time\n",
    "                epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "                train_loss_list.append(train_loss)\n",
    "                \n",
    "                print(f\"Epoch: {epoch+1:02} | Time {epoch_mins}m {epoch_secs}s| lr: {optimizer.param_groups[0]['lr']}\")\n",
    "                print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "                \n",
    "#                 if not valid_iter:\n",
    "#                     valid_iter = test_iter\n",
    "                    \n",
    "                if valid_iter:\n",
    "                    valid_loss = evaluate(model, valid_iter, loss_func)\n",
    "                        \n",
    "                    valid_loss_list.append(valid_loss)\n",
    "\n",
    "                    if valid_loss < best_valid_loss:\n",
    "                        best_valid_loss = valid_loss\n",
    "                        torch.save(model.state_dict(), 'result/{}.pt'.format(model_result_path))\n",
    "\n",
    "                    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}\")\n",
    "\n",
    "                if con.use_CosineAnnealingLR:\n",
    "                    scheduler.step()\n",
    "                elif valid_iter:\n",
    "                    scheduler.step(valid_loss)\n",
    "\n",
    "            torch.save(model.state_dict(), 'result/{}_final.pt'.format(model_result_path))\n",
    "            result_data['total_time'] = total_time\n",
    "            result_data['train_loss_list'] = train_loss_list\n",
    "            result_data['valid_loss_list'] = valid_loss_list\n",
    "            result_data['n_epochs'] = con.N_EPOCHS\n",
    "            with open('result/{}_stat.json'.format(model_result_path), 'w') as f:\n",
    "                f.write(json.dumps(result_data))\n",
    "            # draw\n",
    "            train_valid_loss(train_loss_list, valid_loss_list, model_result_path)\n",
    "            # test\n",
    "            if valid_iter:\n",
    "                predict_evaluate(test_iter, model_result_path, SRC, TGT, con)\n",
    "                predict_evaluate(test_iter, model_result_path, SRC, TGT, con, use_final=True)\n",
    "            else:\n",
    "                predict_evaluate(test_iter, model_result_path, SRC, TGT, con, use_final=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
