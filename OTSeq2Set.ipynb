{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import time, random, math, string\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "SEED = 1225\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "from utils.data import processing_data_EUR_Lex, processing_data_wiki31k, processing_data_Amazon670k, processing_data_AmazonCat13k\n",
    "from utils.plot import train_valid_loss\n",
    "from utils.embedding import src_embedding_glove, tgt_embedding_glove\n",
    "from model.seq2seq import Seq2Seq\n",
    "from model.seq2seq_conv import Seq2Seq as Seq2SeqConv\n",
    "from model.loss import DynamicHungarianLossAssignAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 0\n",
    "device = torch.device('cuda:{}'.format(gpu_id) if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name or \"bias\" in name:\n",
    "            nn.init.uniform_(param.data, -0.1, 0.1)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def train(model, iterator, optimizer, loss_func, teacher_forcing_ratio, clip = None, lambda_embedding=0.1):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        src, lengths = batch.src\n",
    "        tgt = batch.tgt\n",
    "        # src = [src_len, batch_size]\n",
    "        # tgt = [tgt_len, batch_size]\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if isinstance(loss_func, nn.CrossEntropyLoss):\n",
    "            output = model(src, lengths, tgt, teacher_forcing_ratio)\n",
    "            output_dim = output.shape[-1]\n",
    "            # output = [tgt_len, batch_size, output_dim]\n",
    "            # transfrom output : flatten the output into 2 dim.\n",
    "            output = output.view(-1, output_dim)\n",
    "            tgt = tgt[1:].view(-1) # exclude SOS\n",
    "            # output = [tgt_len * batch_size, output_dim]\n",
    "            # tgt = [tgt_len * batch_size]\n",
    "            loss = loss_func(output, tgt)\n",
    "        elif isinstance(loss_func, DynamicHungarianLossAssignAll):\n",
    "            output = model(src, lengths, tgt, teacher_forcing_ratio)\n",
    "            output = output.transpose(0,1)\n",
    "            tgt = tgt[1:].transpose(0,1)\n",
    "            loss = loss_func(output, tgt, model.decoder.embedding)\n",
    "                \n",
    "        loss.backward()\n",
    "        # clip\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, loss_func, lambda_embedding=0.1):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, batch in enumerate(iterator):\n",
    "            \n",
    "            src, lengths= batch.src\n",
    "            tgt = batch.tgt\n",
    "            \n",
    "            if isinstance(loss_func, nn.CrossEntropyLoss):\n",
    "                output = model(src, lengths, tgt, 0) # turn off teacher forcing.\n",
    "                output_dim = output.shape[-1]\n",
    "                # output = [tgt_len, batch_size, output_dim]\n",
    "                # transfrom output : flatten the output into 2 dim.\n",
    "                output = output.view(-1, output_dim)\n",
    "                tgt = tgt[1:].view(-1) # exclude SOS\n",
    "                # output = [tgt_len * batch_size, output_dim]\n",
    "                # tgt = [tgt_len * batch_size]\n",
    "                loss = loss_func(output, tgt)\n",
    "            elif isinstance(loss_func, DynamicHungarianLossAssignAll):\n",
    "                output = model(src, lengths, tgt, 0) # turn off teacher forcing.\n",
    "                output = output.transpose(0,1)\n",
    "                tgt = tgt[1:].transpose(0,1)\n",
    "                loss = loss_func(output, tgt, model.decoder.embedding)\n",
    "                    \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# a function that used to tell us how long an epoch takes.\n",
    "def epoch_time(start_time, end_time):\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time  / 60)\n",
    "    elapsed_secs = int(elapsed_time -  (elapsed_mins * 60))\n",
    "    return  elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_evaluate(iterator, model_result_path, SRC, TGT, conf, suffix=\"\", *, use_final=False):\n",
    "    INIT_IDX = TGT.vocab.stoi[TGT.init_token]\n",
    "    PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "    EOS_IDX = TGT.vocab.stoi[TGT.eos_token]\n",
    "    \n",
    "    if hasattr(conf, \"dl_conv\") and conf.dl_conv:\n",
    "        model = Seq2SeqConv(conf)\n",
    "    else:\n",
    "        model = Seq2Seq(conf)\n",
    "        \n",
    "    if use_final:\n",
    "        suffix +=\"_final\"\n",
    "        model.load_state_dict(torch.load('result/{}_final.pt'.format(model_result_path), map_location=f\"cuda:{gpu_id}\"))\n",
    "    else:\n",
    "        model.load_state_dict(torch.load('result/{}.pt'.format(model_result_path), map_location=f\"cuda:{gpu_id}\"))\n",
    "    \n",
    "    model.to(device)\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    tgt_true_all = []\n",
    "    tgt_pred_all = []\n",
    "    \n",
    "    predict_file = f\"result/{model_result_path}_test{suffix}.txt\"\n",
    "    if os.path.exists(predict_file):\n",
    "        os.remove(predict_file)\n",
    "    \n",
    "    # evalute metrics\n",
    "    TP = torch.zeros(len(TGT.vocab), dtype=torch.long, device=device)\n",
    "    FP = torch.zeros(len(TGT.vocab), dtype=torch.long, device=device)\n",
    "    FN = torch.zeros(len(TGT.vocab), dtype=torch.long, device=device)\n",
    "    \n",
    "    output_dim = len(TGT.vocab)\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for k, batch in enumerate(iterator):\n",
    "            \n",
    "            src, lengths = batch.src\n",
    "            tgt = batch.tgt\n",
    "            # src : [seq_len, batch_size]\n",
    "                \n",
    "            result = None\n",
    "            \n",
    "            output = model.sample(src, lengths, INIT_IDX, PAD_IDX, EOS_IDX, conf.mask)\n",
    "            # output = [seq_len, batch_size, output_dim]\n",
    "            output = output.argmax(2).T\n",
    "            # output: [batch_size, seq_len]\n",
    "            result = output.cpu().numpy().tolist()\n",
    "            \n",
    "            src = src.T.cpu().numpy().tolist()\n",
    "            tgt_tensor = tgt[1:].T\n",
    "            # tgt_tensor: [batch_size, seq_len]\n",
    "            tgt = tgt_tensor.cpu().numpy().tolist()\n",
    "            \n",
    "            with open(predict_file, 'a') as f:\n",
    "                for i in range(len(result)):\n",
    "                    # symptom\n",
    "                    src_one = src[i]\n",
    "                    src_true = []\n",
    "#                     for j in range(len(src_one)):\n",
    "#                         src_true.append(SRC.vocab.itos[src_one[j]])\n",
    "                    # true labels\n",
    "                    tgt_true_one = tgt[i]\n",
    "                    tgt_true = []\n",
    "                    for k in range(len(tgt_true_one)):\n",
    "                        tgt_true.append(TGT.vocab.itos[tgt_true_one[k]])\n",
    "                    # predicted herb\n",
    "                    one = result[i]\n",
    "                    tgt_result=[]\n",
    "                    for j in range(len(one)):\n",
    "                        tgt_result.append(TGT.vocab.itos[one[j]])\n",
    "                    f.write(\"{} | {} | {} \\n\".format(\" \".join(src_true), \" \".join(tgt_true), \" \".join(tgt_result)))\n",
    "                    \n",
    "                    output_i = output[i]\n",
    "                    first_eos_index_o = (output_i==EOS_IDX).nonzero(as_tuple=True)[0]\n",
    "                    if first_eos_index_o.size()[0] > 0:\n",
    "                        first_eos_index_o = first_eos_index_o[0].item()\n",
    "                        output_i = output_i[:first_eos_index_o]\n",
    "                        \n",
    "                    output_i_onehot = torch.zeros(output_dim, dtype=torch.bool, device=device).scatter_(0, output_i, 1)\n",
    "                    \n",
    "                    tgt_tensor_i = tgt_tensor[i]\n",
    "                    first_eos_index_t = (tgt_tensor_i==EOS_IDX).nonzero(as_tuple=True)[0]\n",
    "                    if first_eos_index_t.size()[0] > 0:\n",
    "                        first_eos_index_t = first_eos_index_t[0].item()\n",
    "                        tgt_tensor_i = tgt_tensor_i[:first_eos_index_t]\n",
    "                    \n",
    "                    tgt_tensor_i_onehot = torch.zeros(output_dim, dtype=torch.bool, device=device).scatter_(0, tgt_tensor_i, 1)\n",
    "                    # for micro f1 score\n",
    "                    TP_i = torch.logical_and(output_i_onehot, tgt_tensor_i_onehot, out=torch.empty(output_dim, dtype=torch.bool, device=device))\n",
    "                    TP = TP + TP_i\n",
    "                    FP_i = torch.logical_and(output_i_onehot, torch.logical_not(tgt_tensor_i_onehot), out=torch.empty(output_dim, dtype=torch.bool, device=device))\n",
    "                    FP = FP + FP_i\n",
    "                    FN_i = torch.logical_and(torch.logical_not(output_i_onehot), tgt_tensor_i_onehot, out=torch.empty(output_dim, dtype=torch.bool, device=device))\n",
    "                    FN = FN + FN_i\n",
    "\n",
    "    # ignore pad\n",
    "    TP[PAD_IDX] = 0\n",
    "    FP[PAD_IDX] = 0\n",
    "    FN[PAD_IDX] = 0\n",
    "    TP_sum = torch.sum(TP).item()\n",
    "    f1_micro = 2*TP_sum / (2*TP_sum + torch.sum(FP).item() + torch.sum(FN).item())\n",
    "    precision_micro = TP_sum / (TP_sum + torch.sum(FP).item())\n",
    "    recall_micro = TP_sum / (TP_sum + torch.sum(FN).item())\n",
    "\n",
    "    \n",
    "    evaluation = f\"f1_micro{f1_micro}\\n precision_micro:{precision_micro}\\n recall_micro:{recall_micro}\"\n",
    "    with open(f\"result/{model_result_path}_evaluation{suffix}_new.txt\",'w') as f:\n",
    "        f.write(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute weight\n",
    "def class_balance(freq, beta=0.9):\n",
    "    return (1-beta)/(1-beta**freq)\n",
    "\n",
    "def get_class_balanced_weight(TGT, beta=0.9):\n",
    "    vocab_size = len(TGT.vocab)\n",
    "    weight = torch.ones(vocab_size, device=device)\n",
    "\n",
    "    for i in range(vocab_size):\n",
    "        token = TGT.vocab.itos[i]\n",
    "        if token == TGT.unk_token or token == TGT.pad_token or token == TGT.init_token:\n",
    "            weight[i] = (1-beta)*0.2\n",
    "        else:\n",
    "            weight[i] = class_balance(TGT.vocab.freqs[token], beta)\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config/OTSeq2Set.json\", 'r') as f:\n",
    "    config = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Struct:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:EUR-Lex,model:seq2seq_attention:1\n",
      "pad_idx:1,init_idx:2,eos_idx:3\n",
      "tgt_embedding use: glove\n",
      "Seq2Seq(\n",
      "  (encoder): rnn_encoder(\n",
      "    (embedding): Embedding(50002, 300)\n",
      "    (rnn): GRU(300, 512, num_layers=2, dropout=0.2, bidirectional=True)\n",
      "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (decoder): rnn_decoder(\n",
      "    (embedding): Embedding(3960, 300)\n",
      "    (rnn): StackedGRU(\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): GRUCell(1324, 512)\n",
      "      )\n",
      "    )\n",
      "    (fc_out): Linear(in_features=1836, out_features=3960, bias=True)\n",
      "    (attention): bahdanau_attention(\n",
      "      (linear_encoder): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (linear_decoder): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (linear_v): Linear(in_features=512, out_features=1, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "The model has 34,824,401 trainable parameters\n",
      "Epoch: 01 | Time 1m 54s| lr: 0.001\n",
      "\tTrain Loss: 4.695 | Train PPL: 109.427\n",
      "Epoch: 02 | Time 1m 55s| lr: 0.0009938441702975688\n",
      "\tTrain Loss: 3.084 | Train PPL:  21.844\n",
      "Epoch: 03 | Time 1m 55s| lr: 0.0009755282581475768\n",
      "\tTrain Loss: 2.324 | Train PPL:  10.215\n",
      "Epoch: 04 | Time 1m 55s| lr: 0.0009455032620941839\n",
      "\tTrain Loss: 1.784 | Train PPL:   5.952\n"
     ]
    }
   ],
   "source": [
    "for data_name, conf_list in config.items():\n",
    "    \n",
    "    if data_name == \"EUR-Lex\":\n",
    "        processing_data = processing_data_EUR_Lex\n",
    "    elif data_name == \"Wiki31k\":\n",
    "        processing_data = processing_data_wiki31k\n",
    "    elif data_name ==\"Amazon670k\":\n",
    "        processing_data = processing_data_Amazon670k\n",
    "    elif data_name == \"AmazonCat13k\":\n",
    "        processing_data = processing_data_AmazonCat13k\n",
    "    else:\n",
    "        raise Exception(\"Data set don't exists!\")\n",
    "\n",
    "    for conf in conf_list:\n",
    "        if conf.get(\"finish\") == False:\n",
    "            con = Struct(**conf)\n",
    "            print(f\"dataset:{data_name},model:{con.model}:{con.model_number}\")\n",
    "            # load data\n",
    "            for_CE = True if con.loss_func in (\"CE\", \"DynamicHungarianLoss\") else False\n",
    "            \n",
    "            tgt_sort = True if hasattr(con, \"tgt_sort\") and con.tgt_sort else False\n",
    "            \n",
    "            if hasattr(con, \"max_src_len\"):\n",
    "                train_iter, valid_iter, test_iter, SRC, TGT = processing_data(device, max_src_len=con.max_src_len, include_lengths=True, batch_size=con.batch_size,\n",
    "                                                                          for_CE=for_CE, valid_split=con.valid_split, tgt_sort=tgt_sort)\n",
    "            else:\n",
    "                train_iter, valid_iter, test_iter, SRC, TGT = processing_data(device, include_lengths=True, batch_size=con.batch_size,\n",
    "                                                                              for_CE=for_CE, valid_split=con.valid_split, tgt_sort=tgt_sort)\n",
    "                                                                        \n",
    "            PAD_IDX = TGT.vocab.stoi[TGT.pad_token]\n",
    "            INIT_IDX = TGT.vocab.stoi[TGT.init_token]\n",
    "            EOS_IDX = TGT.vocab.stoi[TGT.eos_token]\n",
    "            print(f\"pad_idx:{PAD_IDX},init_idx:{INIT_IDX},eos_idx:{EOS_IDX}\")\n",
    "\n",
    "            \n",
    "            INPUT_DIM = len(SRC.vocab)\n",
    "            OUTPUT_DIM = len(TGT.vocab)\n",
    "            con.src_vocab_size = INPUT_DIM\n",
    "            con.tgt_vocab_size = OUTPUT_DIM\n",
    "            \n",
    "            # initialize model.\n",
    "            if hasattr(con, \"dl_conv\") and con.dl_conv:\n",
    "                model = Seq2SeqConv(con).to(device)\n",
    "            else:\n",
    "                model = Seq2Seq(con).to(device)\n",
    "            model.apply(init_weights)\n",
    "            \n",
    "            if con.src_glove:\n",
    "                model.encoder.embedding = src_embedding_glove(device, SRC)\n",
    "\n",
    "            if con.tgt_embedding == \"glove\":\n",
    "                print(\"tgt_embedding use: glove\")\n",
    "                model.decoder.embedding = tgt_embedding_glove(device, TGT)\n",
    "                \n",
    "            print(model)\n",
    "            print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "            if hasattr(model.decoder, \"bottleneck1\"):\n",
    "                print(f'The bottleneck1 has {count_parameters(model.decoder.bottleneck1):,} trainable parameters')\n",
    "                \n",
    "            if hasattr(model, \"dl_conv\"):\n",
    "                print(f'The dl_conv has {count_parameters(model.dl_conv):,} trainable parameters')\n",
    "                \n",
    "            optimizer = optim.Adam(model.parameters(), lr=con.learning_rate)\n",
    "            \n",
    "            if con.use_CosineAnnealingLR:\n",
    "                scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=con.N_EPOCHS)\n",
    "            else:\n",
    "                scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                                 'min', \n",
    "                                                                 factor=0.5,\n",
    "                                                                 patience=3,\n",
    "                                                                 verbose=1,\n",
    "                                                                 min_lr = 1e-10)\n",
    "            # loss func\n",
    "            if con.loss_func == \"CE\":\n",
    "                loss_func = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "            elif con.loss_func == \"DynamicHungarianLossAssignAll\":\n",
    "                loss_func = DynamicHungarianLossAssignAll(PAD_IDX, con.ignore_index, con.assign_all_pre, con.empty_weight, con.lambda_embedding,\n",
    "                                                         con.ipot_E_non_empty, con.ipot_E_first_n_pre)\n",
    "                \n",
    "            model_name = \"_\".join([data_name, con.model, con.loss_func])\n",
    "            # train\n",
    "            model_dir = f\"result/{model_name}\"\n",
    "            if not os.path.exists(model_dir):\n",
    "                os.mkdir(model_dir)\n",
    "            model_result_path = f\"{model_name}/{con.model_number}\"\n",
    "            best_valid_loss = float('inf')\n",
    "\n",
    "            train_loss_list = []\n",
    "            valid_loss_list = []\n",
    "            result_data = {}\n",
    "            total_time = 0\n",
    "            for epoch in range(con.N_EPOCHS):\n",
    "\n",
    "                start_time = time.time()\n",
    "                \n",
    "                train_loss = train(model, train_iter, optimizer, loss_func, con.teacher_forcing_ratio, con.CLIP)\n",
    "\n",
    "                end_time = time.time()\n",
    "                \n",
    "                total_time += end_time - start_time\n",
    "                epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "                train_loss_list.append(train_loss)\n",
    "                \n",
    "                print(f\"Epoch: {epoch+1:02} | Time {epoch_mins}m {epoch_secs}s| lr: {optimizer.param_groups[0]['lr']}\")\n",
    "                print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "                \n",
    "#                 if not valid_iter:\n",
    "#                     valid_iter = test_iter\n",
    "                    \n",
    "                if valid_iter:\n",
    "                    valid_loss = evaluate(model, valid_iter, loss_func)\n",
    "                        \n",
    "                    valid_loss_list.append(valid_loss)\n",
    "\n",
    "                    if valid_loss < best_valid_loss:\n",
    "                        best_valid_loss = valid_loss\n",
    "                        torch.save(model.state_dict(), 'result/{}.pt'.format(model_result_path))\n",
    "\n",
    "                    print(f\"\\tValid Loss: {valid_loss:.3f} | Valid PPL: {math.exp(valid_loss):7.3f}\")\n",
    "\n",
    "                if con.use_CosineAnnealingLR:\n",
    "                    scheduler.step()\n",
    "                elif valid_iter:\n",
    "                    scheduler.step(valid_loss)\n",
    "\n",
    "            torch.save(model.state_dict(), 'result/{}_final.pt'.format(model_result_path))\n",
    "            result_data['total_time'] = total_time\n",
    "            result_data['train_loss_list'] = train_loss_list\n",
    "            result_data['valid_loss_list'] = valid_loss_list\n",
    "            result_data['n_epochs'] = con.N_EPOCHS\n",
    "            with open('result/{}_stat.json'.format(model_result_path), 'w') as f:\n",
    "                f.write(json.dumps(result_data))\n",
    "            # draw\n",
    "            train_valid_loss(train_loss_list, valid_loss_list, model_result_path)\n",
    "            # test\n",
    "            if valid_iter:\n",
    "                predict_evaluate(test_iter, model_result_path, SRC, TGT, con)\n",
    "                predict_evaluate(test_iter, model_result_path, SRC, TGT, con, use_final=True)\n",
    "            else:\n",
    "                predict_evaluate(test_iter, model_result_path, SRC, TGT, con, use_final=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xmlc_cj",
   "language": "python",
   "name": "xmlc_cj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
